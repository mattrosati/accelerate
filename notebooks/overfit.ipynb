{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b4acb5",
   "metadata": {},
   "source": [
    "## Overfitting Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae48fb9",
   "metadata": {},
   "source": [
    "### Useful Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd781dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")  # add project root\n",
    "\n",
    "import shutil\n",
    "import re\n",
    "from argparse import ArgumentParser\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import zarr\n",
    "import dask.array as da\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "from src.data_utils import *\n",
    "from src.constants import *\n",
    "from src.tuner import train_cv, RayAdaptiveRepeatedCVSearch\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:10,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8214fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47434e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path constants\n",
    "train_dir = \"/home/mr2238/scratch_pi_np442/mr2238/accelerate/total/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a603ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check img directory exists, if not make it\n",
    "img_dir = \"/home/mr2238/project_pi_np442/mr2238/accelerate/imgs/overfit\"\n",
    "os.makedirs(img_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4bb7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009273e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"smooth_downsample_w_300s_hr_rso2r_rso2l_spo2_abp\"\n",
    "run_name = \"2.0rapid\"\n",
    "small = False\n",
    "model_name = f\"models{'_debug' if small else ''}_{run_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = os.path.join(train_dir, dataset_name, model_name)\n",
    "print(model_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(model_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_states = {}\n",
    "for f in os.listdir(model_store):\n",
    "    if not f.endswith(\".pkl\"):\n",
    "        state = tune.ExperimentAnalysis(experiment_checkpoint_path=os.path.join(model_store, f))\n",
    "        model_states[f] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3faf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD load test metrics? could also just move this to eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b52ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plot Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model_states.items():\n",
    "    print(k)\n",
    "    print(v.results_df.columns[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8518cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather results\n",
    "def gather_results(model_states, metric, others_to_fetch):\n",
    "    rows = []\n",
    "    of_interest = ['model'] + [metric] + others_to_fetch\n",
    "    for k, v in model_states.items():\n",
    "        df = v.results_df\n",
    "        try:\n",
    "            result = df.loc[[df[metric].idxmax()]]\n",
    "            result[\"model\"] = k\n",
    "            rows.append(result[of_interest])\n",
    "        except:\n",
    "            continue\n",
    "    return pd.concat(rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ae576",
   "metadata": {},
   "outputs": [],
   "source": [
    "others = ['mean_train_auc', 'std_val_auc', 'std_train_auc',\n",
    "       'mean_val_auc', 'mean_val_balanced_accuracy',\n",
    "       'std_val_balanced_accuracy', 'std_train_balanced_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = gather_results(model_states, 'mean_train_balanced_accuracy', others)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908c2f4",
   "metadata": {},
   "source": [
    "### Plot results per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119cd008",
   "metadata": {},
   "source": [
    "##### Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list training dirs\n",
    "dataset_names = os.listdir(train_dir)\n",
    "run_name = \"2.0rapid\"\n",
    "len(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f698321",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"debug\" in dataset_names:\n",
    "    dataset_names.remove(\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = os.path.join(img_dir, run_name)\n",
    "os.makedirs(img_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b219e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through training dirs, pick out training results per model\n",
    "def model_path_iter(dataset_names, run_name):\n",
    "    for ds in dataset_names:\n",
    "        ds_path = os.path.join(train_dir, ds)\n",
    "        for model_dir in os.listdir(ds_path):\n",
    "            if run_name in model_dir:\n",
    "                # grab model paths\n",
    "                md_path = os.path.join(ds_path, model_dir)\n",
    "                for m in os.listdir(md_path):\n",
    "                    model_path = os.path.join(md_path, m)\n",
    "                    if not m.endswith(\".pkl\"):\n",
    "                        yield model_path, \"debug\" in model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23155d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = ['mean_val_auc', 'mean_train_auc', 'std_val_auc', 'std_train_auc',\n",
    "       'mean_train_balanced_accuracy', 'mean_val_balanced_accuracy',\n",
    "       'std_val_balanced_accuracy', 'std_train_balanced_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1288dc",
   "metadata": {},
   "source": [
    "##### DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = \"mean_val_auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_results = []\n",
    "for m, d in model_path_iter(dataset_names, run_name):\n",
    "    # grab results_df\n",
    "    try:\n",
    "        state = tune.ExperimentAnalysis(experiment_checkpoint_path=m)\n",
    "    except ValueError:\n",
    "        print(f\"Could not find experiment at {m}, skipping.\")\n",
    "        continue\n",
    "    df = state.results_df\n",
    "    if df.shape[1] > 0:\n",
    "        df = df[df['done'] == True]\n",
    "        df = df[of_interest]\n",
    "    \n",
    "        # add debug flag to df\n",
    "        df['debug'] = d\n",
    "        # add model_name\n",
    "        df['model'] = os.path.basename(m)\n",
    "\n",
    "        # add dataset_name\n",
    "        df['dataset'] = os.path.basename(os.path.dirname(os.path.dirname(m)))\n",
    "\n",
    "        # select the best based on mean_val_auc\n",
    "        best_row = df.loc[df[selector].idxmax()].to_frame().T\n",
    "\n",
    "        # combine into one dataset\n",
    "        large_results.append(best_row)\n",
    "\n",
    "large_result_df = pd.concat(large_results)\n",
    "print(large_result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36738c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(large_result_df.model.unique())\n",
    "len(large_result_df.model.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"separate_decomp\": \"separatepca\", \n",
    "    \"pca\": \"pca\", \n",
    "    \"raw\": \"raw\", \n",
    "    \"chronos\": \"chronos\", \n",
    "    \"design\": \"design\", \n",
    "    \"design_w\": \"whiten\"}\n",
    "modelnames = [\n",
    "            \"log_reg\",\n",
    "            \"svm\",\n",
    "            \"knn\",\n",
    "            \"rand_forest\",\n",
    "            \"decision_tree\",\n",
    "            \"xgb\",\n",
    "            \"rocket\",\n",
    "            \"kn_multivar\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df[\"name_m\"] = large_result_df[\"model\"]\n",
    "large_result_df[\"model\"] = large_result_df[\"model\"].str.replace(\"_separate_pca\", \"_separate_decomp\")\n",
    "for k, v in mapping.items():\n",
    "    large_result_df[\"model\"] = large_result_df[\"model\"].str.replace(pat=k, repl=v)\n",
    "large_result_df = large_result_df.rename(columns={\"model\": \"model_name\"})\n",
    "pprint(large_result_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df[[\"model\", \"datamode\"]] = (\n",
    "    large_result_df[\"model_name\"]\n",
    "      .str.rsplit(\"_\", n=1, expand=True)\n",
    ")\n",
    "large_result_df.drop(columns=\"model_name\")\n",
    "large_result_df[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(large_result_df[\"model\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(large_result_df[\"datamode\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = large_result_df.melt(\n",
    "    id_vars=[\"dataset\", \"debug\", \"model\", \"datamode\"],\n",
    "    value_vars=[\"mean_train_auc\", \"mean_val_auc\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"auc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90582874",
   "metadata": {},
   "source": [
    "##### Graph DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all model performances on scatter plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for embedding in large_result_df.datamode.unique():\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    d = large_result_df[large_result_df.datamode == embedding].sort_values(by=[\"mean_val_auc\", \"mean_train_auc\"], ascending=False)\n",
    "    g = sns.scatterplot(\n",
    "        data=d,\n",
    "        x = \"mean_train_auc\",\n",
    "        y = \"mean_val_auc\",\n",
    "        hue=\"model\",\n",
    "        style=\"dataset\",\n",
    "        s=150,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    # add y=x line\n",
    "    plt.plot([0, 1], [0, 1], ls='--', c='gray')\n",
    "    g.set_title(f\"{embedding.upper()} model performances\")\n",
    "    g.set_xlabel(\"Mean Train AUC\")\n",
    "    g.set_ylabel(\"Mean Val AUC\")\n",
    "    g.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
    "    g.set_ylim(0.3, 0.8)\n",
    "    g.set_xlim(0.5, 1.0)\n",
    "    \n",
    "    img_name = f\"{embedding}_all_small_models_performance.png\"\n",
    "    plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05fa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot models in val AUC vs val balanced acc\n",
    "plt.figure(figsize=(10, 5))\n",
    "for embedding in large_result_df.datamode.unique():\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    d = large_result_df[large_result_df.datamode == embedding]\n",
    "    g = sns.scatterplot(\n",
    "        data=d,\n",
    "        x = \"mean_val_auc\",\n",
    "        y = \"mean_val_balanced_accuracy\",\n",
    "        hue=\"model\",\n",
    "        style=\"dataset\",\n",
    "        s=150,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    # add y=x line\n",
    "    # plt.plot([0, 1], [0, 1], ls='--', c='gray')\n",
    "    g.set_title(f\"{embedding.upper()} model performances\")\n",
    "    g.set_xlabel(\"Mean Val AUC\")\n",
    "    g.set_ylabel(\"Mean Val Balanced Accuracy\")\n",
    "    g.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
    "    g.set_ylim(0.45, 0.7)\n",
    "    g.set_xlim(0.45, 0.7)\n",
    "    \n",
    "    img_name = f\"{embedding}_all_small_models_pareto.png\"\n",
    "    plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca700f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot models in val AUC vs val balanced acc\n",
    "plt.figure(figsize=(16, 12))\n",
    "d = large_result_df\n",
    "g = sns.scatterplot(\n",
    "    data=d,\n",
    "    x = \"mean_val_auc\",\n",
    "    y = \"mean_val_balanced_accuracy\",\n",
    "    hue=\"model\",\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "# add y=x line\n",
    "# plt.plot([0, 1], [0, 1], ls='--', c='gray')\n",
    "# g.set_title(f\"Model performances AUC-accuracy Tradeoff\")\n",
    "g.set_xlabel(\"Mean Validation AUROC\")\n",
    "g.set_ylabel(\"Mean Validation Balanced Accuracy\")\n",
    "g.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
    "# g.set_ylim(0.45, 0.7)\n",
    "# g.set_xlim(0.45, 0.7)\n",
    "\n",
    "img_name = f\"all_small_models_pareto.png\"\n",
    "plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all model performances on scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "d = large_result_df\n",
    "g = sns.scatterplot(\n",
    "    data=d,\n",
    "    x = \"mean_train_auc\",\n",
    "    y = \"mean_val_auc\",\n",
    "    hue=\"model\",\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "# add y=x line\n",
    "plt.plot([0, 1], [0, 1], ls='--', c='gray')\n",
    "# g.set_title(f\"{embedding.upper()} model performances\")\n",
    "g.set_xlabel(\"Mean Training AUROC\")\n",
    "g.set_ylabel(\"Mean Validation AUROC\")\n",
    "g.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
    "g.set_ylim(0.3, 0.8)\n",
    "g.set_xlim(0.45, 1.0)\n",
    "\n",
    "img_name = f\"all_small_models_performance.png\"\n",
    "plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa04ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "for ds in long_df.dataset.unique():\n",
    "    plot_df = long_df[(long_df.dataset == ds)].copy()\n",
    "    plot_df[\"model\"] = np.where(plot_df[\"debug\"], plot_df[\"model\"] + \"*\",plot_df[\"model\"])\n",
    "    # plot grouped barchart of train_auc and val_auc with model on x axis\n",
    "    if plot_df.empty:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    g = sns.catplot(\n",
    "        data=plot_df,\n",
    "        x=\"datamode\",\n",
    "        y=\"auc\",\n",
    "        hue=\"metric\",\n",
    "        col=\"model\",\n",
    "        kind=\"bar\",\n",
    "        dodge=True,\n",
    "        height=4,\n",
    "        aspect=1.2,\n",
    "        col_wrap = 3,\n",
    "        sharex=False,\n",
    "    )\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    # g.set_xticklabels(rotation=30)\n",
    "    g.set_axis_labels(\"\", \"AUC\")\n",
    "    g.set(ylim=(0, 1))\n",
    "    # g.legend.set_loc(\"upper right\")\n",
    "\n",
    "    \n",
    "    plt.suptitle(f\"{ds}\", y=1.04)\n",
    "    # plt.tight_layout()\n",
    "    # plt.legend(loc=(1,1))\n",
    "    img_name = f\"{ds}.png\"\n",
    "    # plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top 5 per group\n",
    "for s in long_df.metric.unique():\n",
    "    print(f\"Top 5 for {s}:\")\n",
    "    print(long_df[long_df.metric == s].sort_values(by=\"auc\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae19a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(large_result_df.sort_values(by=[\"mean_val_auc\", \"mean_train_auc\"], ascending=False).dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5efc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which datasets have highest performance\n",
    "groups = large_result_df.groupby(['dataset'])\n",
    "print(groups['mean_val_balanced_accuracy'].max().sort_values(ascending=False)[:50])\n",
    "print(groups['mean_val_auc'].mean().sort_values(ascending=False)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which datasets have highest performance\n",
    "groups = large_result_df.groupby(['model', 'dataset'])\n",
    "of_interest = groups['mean_val_auc'].max().sort_values(ascending=False).reset_index()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(of_interest[of_interest.model == \"xgb\"].dataset[:5])\n",
    "print(of_interest[of_interest.model == \"rocket\"].dataset[:5])\n",
    "print(of_interest[of_interest.model == \"rand_forest\"].dataset[:5])\n",
    "# print(groups['mean_val_auc'].mean().sort_values(ascending=False)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215fd95",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2888a43",
   "metadata": {},
   "source": [
    "\n",
    "#### Pareto Frontier Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareto frontier\n",
    "def identify_pareto(scores):\n",
    "    # Count number of items\n",
    "    population_size = scores.shape[0]\n",
    "    # Create a NumPy index for scores on the Pareto front (zero indexed)\n",
    "    pareto_front = np.ones(population_size, dtype=bool)\n",
    "    # Compare each point with all others\n",
    "    for i in range(population_size):\n",
    "        for j in range(population_size):\n",
    "            # Check if point 'i' is dominated by point 'j'\n",
    "            if all(scores[j] >= scores[i]) and any(scores[j] > scores[i]):\n",
    "                # Point 'i' is dominated, thus not on Pareto front\n",
    "                pareto_front[i] = 0\n",
    "                break\n",
    "    # Return indices of Pareto front\n",
    "    return pareto_front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b383210",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df['is_pareto'] = identify_pareto(large_result_df[[\"mean_val_auc\", \"mean_val_balanced_accuracy\"]].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df['ci_val_auc'] = (large_result_df['std_val_auc'] / np.sqrt(15)) * 1.96\n",
    "large_result_df['ci_val_balanced_accuracy'] = (large_result_df['std_val_balanced_accuracy'] / np.sqrt(15)) * 1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df.sort_values(by=[\"mean_val_auc\"], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df[\"pareto\"] = large_result_df[\"mean_val_auc\"] + large_result_df['ci_val_auc'] + large_result_df['ci_val_balanced_accuracy'] + large_result_df[\"mean_val_balanced_accuracy\"]\n",
    "large_result_df.sort_values(by=[\"pareto\"], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad12628",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df[\"is_pareto\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130326a8",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_confusion(row, base_dir=train_dir, run_name=run_name):\n",
    "    models_dir_name = \"models_debug\" if bool(row.debug) else \"models\"\n",
    "    models_dir_name = f\"{models_dir_name}_{run_name}\"\n",
    "    model_path = os.path.join(base_dir, row.dataset, models_dir_name)\n",
    "    print(f\"loading from: {model_path}\")\n",
    "\n",
    "    model_name = str(row.name_m)\n",
    "\n",
    "\n",
    "    return load(\n",
    "        open(\n",
    "            os.path.join(model_path, f\"{model_name}_confmat.pkl\"),\n",
    "            \"rb\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = load_confusion(row=large_result_df.sort_values(by=[\"pareto\"], ascending=False).iloc[0])\n",
    "print(conf / conf.sum())\n",
    "conf = load_confusion(row=large_result_df.sort_values(by=[\"mean_val_auc\"], ascending=False).iloc[0])\n",
    "print(conf / conf.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8ae38",
   "metadata": {},
   "source": [
    "#### Running Best Models on Whole Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(row, base_dir=train_dir, run_name=run_name):\n",
    "    models_dir_name = \"models_debug\" if bool(row.debug) else \"models\"\n",
    "    models_dir_name = f\"{models_dir_name}_{run_name}\"\n",
    "    model_path = os.path.join(base_dir, row.dataset, models_dir_name)\n",
    "    print(f\"loading from: {model_path}\")\n",
    "\n",
    "    model_name = str(row.name_m)\n",
    "\n",
    "\n",
    "    return load(\n",
    "        open(\n",
    "            os.path.join(model_path, f\"{model_name}.pkl\"),\n",
    "            \"rb\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_models = large_result_df[large_result_df[\"is_pareto\"]].copy()\n",
    "pareto_models.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = []\n",
    "for i in range(pareto_models.shape[0]):\n",
    "    m = load_model(pareto_models.iloc[i])\n",
    "    m_list.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9affd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "def get_data(row, base_dir=train_dir):\n",
    "    data_mode = row.datamode\n",
    "    print(data_mode)\n",
    "    if data_mode == \"raw\":\n",
    "        f = \"x.zarr\"\n",
    "    elif data_mode == \"pca\":\n",
    "        f = \"pca_x.zarr\"\n",
    "    elif data_mode == \"fpca\":\n",
    "        f = \"fpca_x.zarr\"\n",
    "    elif data_mode == \"separatepca\":\n",
    "        f = \"separate_decomp_x.zarr\"\n",
    "    elif data_mode == \"chronos\":\n",
    "        f = \"chronos_x.zarr\"\n",
    "    elif data_mode == \"design\":\n",
    "        f = \"design_x.zarr\"\n",
    "    elif data_mode == \"whiten\":\n",
    "        f = \"white_design_x.zarr\"\n",
    "    X_train = da.from_zarr(os.path.join(train_dir, row.dataset, \"permanent\", \"train\", f))\n",
    "    labels = pd.read_pickle(\n",
    "        os.path.join(train_dir, row.dataset, \"permanent\", \"train\", \"labels.pkl\")\n",
    "    )\n",
    "    y_train = labels[\"in?\"].astype(int)\n",
    "\n",
    "    X_test = da.from_zarr(os.path.join(train_dir, row.dataset, \"permanent\", \"test\", f))\n",
    "    labels = pd.read_pickle(\n",
    "        os.path.join(train_dir, row.dataset, \"permanent\", \"test\", \"labels.pkl\")\n",
    "    )\n",
    "    y_test = labels[\"in?\"].astype(int)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "test_auc = []\n",
    "\n",
    "for i in range(pareto_models.shape[0]):\n",
    "    row = pareto_models.iloc[i]\n",
    "    X_train, y_train, X_test, y_test = get_data(row)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    m = m_list[i]\n",
    "    if row.model == \"svm\":\n",
    "        print(f\"  Best params: {m.get_params()}\")\n",
    "        m.set_params(cache_size=1500)\n",
    "    print(m.__class__.__name__)\n",
    "    print(f\"  Best params: {m.get_params()}\")\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "    if hasattr(m, \"predict_proba\"):\n",
    "        y_prob = m.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = m.decision_function(X_test)\n",
    "        y_pred = (y_prob >= 0).astype(int)\n",
    "\n",
    "    test_acc.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    test_auc.append(roc_auc_score(y_test, y_prob))\n",
    "    print(test_acc, test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f1ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    pareto_models[\"test_auc\"] = test_auc\n",
    "    pareto_models[\"test_balanced_accuracy\"] = test_acc\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424241e-37e9-426e-92cf-45943ef4897e",
   "metadata": {},
   "source": [
    "Notice how the balanced accuracy drops drastically, despite the AUROC staying stable. This is likely a result of the decision thresholding. We therefore tune the decision threshold of all these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35619907-6397-4e01-9345-99e533a81c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4df7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_results = []\n",
    "best_acc = []\n",
    "best_thresholds = []\n",
    "ntest_acc = []\n",
    "\n",
    "for i in range(pareto_models.shape[0]):\n",
    "    row = pareto_models.iloc[i]\n",
    "    X_train, y_train, X_test, y_test = get_data(row)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    m = m_list[i]\n",
    "    if row.model == \"svm\":\n",
    "        m.set_params(cache_size=1500)\n",
    "    print(m.__class__.__name__)\n",
    "    print(f\"  Best params: {m.get_params()}\")\n",
    "    tuner = TunedThresholdClassifierCV(estimator = m, \n",
    "                                       cv=None, \n",
    "                                       refit=True, \n",
    "                                       n_jobs=-1, \n",
    "                                       random_state=42, \n",
    "                                       store_cv_results=True,\n",
    "                                      )\n",
    "    tuner.fit(X_train, y_train)\n",
    "\n",
    "    threshold = tuner.best_threshold_\n",
    "    best_acc.append(tuner.best_score_)\n",
    "    best_thresholds.append(threshold)\n",
    "\n",
    "    if hasattr(m, \"predict_proba\"):\n",
    "        y_prob = tuner.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    else:\n",
    "        y_prob = tuner.decision_function(X_test)\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    ntest_acc.append(balanced_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88972790-e056-4e6b-bec9-116bb1aca931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_models[\"tuned_train_acc\"] = best_acc\n",
    "pareto_models[\"tuned_train_thresh\"] = best_thresholds\n",
    "pareto_models[\"tuned_test_acc\"] = ntest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12c5e2-80eb-43e5-8b80-25fb4c175756",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52a0d5-9949-40a6-9387-a5507110a7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
