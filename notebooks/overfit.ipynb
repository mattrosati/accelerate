{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b4acb5",
   "metadata": {},
   "source": [
    "## Overfitting Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae48fb9",
   "metadata": {},
   "source": [
    "### Useful Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd781dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")  # add project root\n",
    "\n",
    "import shutil\n",
    "import re\n",
    "from argparse import ArgumentParser\n",
    "from pickle import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import zarr\n",
    "import dask.array as da\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "from src.data_utils import *\n",
    "from src.constants import *\n",
    "from src.tuner import train_cv, RayAdaptiveRepeatedCVSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:10,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8214fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47434e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path constants\n",
    "train_dir = \"/home/mr2238/project_pi_np442/mr2238/accelerate/data/smooth46/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a603ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check img directory exists, if not make it\n",
    "img_dir = \"/home/mr2238/project_pi_np442/mr2238/accelerate/imgs/overfit\"\n",
    "os.makedirs(img_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4bb7a",
   "metadata": {},
   "source": [
    "### Loading Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009273e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"smooth_downsample_w_300s_hr_rso2r_rso2l_spo2_abp\"\n",
    "run_name = \"current\"\n",
    "small = False\n",
    "model_name = f\"models{'_debug' if small else ''}_{run_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = os.path.join(train_dir, dataset_name, model_name)\n",
    "print(model_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(model_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_states = {}\n",
    "for f in os.listdir(model_store):\n",
    "    if not f.endswith(\".pkl\"):\n",
    "        state = tune.ExperimentAnalysis(experiment_checkpoint_path=os.path.join(model_store, f))\n",
    "        model_states[f] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3faf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD load test metrics? could also just move this to eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b52ad",
   "metadata": {},
   "source": [
    "### Plot Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model_states.items():\n",
    "    print(k)\n",
    "    print(v.results_df.columns[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8518cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather results\n",
    "def gather_results(model_states, metric, others_to_fetch):\n",
    "    rows = []\n",
    "    of_interest = ['model'] + [metric] + others_to_fetch\n",
    "    for k, v in model_states.items():\n",
    "        df = v.results_df\n",
    "        try:\n",
    "            result = df.loc[[df[metric].idxmax()]]\n",
    "            result[\"model\"] = k\n",
    "            rows.append(result[of_interest])\n",
    "        except:\n",
    "            continue\n",
    "    return pd.concat(rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ae576",
   "metadata": {},
   "outputs": [],
   "source": [
    "others = ['mean_train_auc', 'std_val_auc', 'std_train_auc',\n",
    "       'mean_val_auc', 'mean_val_balanced_accuracy',\n",
    "       'std_val_balanced_accuracy', 'std_train_balanced_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = gather_results(model_states, 'mean_train_balanced_accuracy', others)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908c2f4",
   "metadata": {},
   "source": [
    "### Plot results per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119cd008",
   "metadata": {},
   "source": [
    "##### Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list training dirs\n",
    "dataset_names = os.listdir(train_dir)\n",
    "run_name = \"rapid\"\n",
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f698321",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names.remove(\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b219e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through training dirs, pick out training results per model\n",
    "def model_path_iter(dataset_names, run_name):\n",
    "    for ds in dataset_names:\n",
    "        ds_path = os.path.join(train_dir, ds)\n",
    "        for model_dir in os.listdir(ds_path):\n",
    "            if run_name in model_dir:\n",
    "                # grab model paths\n",
    "                md_path = os.path.join(ds_path, model_dir)\n",
    "                for m in os.listdir(md_path):\n",
    "                    model_path = os.path.join(md_path, m)\n",
    "                    if not m.endswith(\".pkl\"):\n",
    "                        yield model_path, \"debug\" in model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23155d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = ['mean_val_auc', 'mean_train_auc', 'std_val_auc', 'std_train_auc',\n",
    "       'mean_train_balanced_accuracy', 'mean_val_balanced_accuracy',\n",
    "       'std_val_balanced_accuracy', 'std_train_balanced_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1288dc",
   "metadata": {},
   "source": [
    "##### DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_results = []\n",
    "for m, d in model_path_iter(dataset_names, run_name):\n",
    "    # grab results_df\n",
    "    try:\n",
    "        state = tune.ExperimentAnalysis(experiment_checkpoint_path=m)\n",
    "    except ValueError:\n",
    "        print(f\"Could not find experiment at {m}, skipping.\")\n",
    "        continue\n",
    "    df = state.results_df\n",
    "    if df.shape[1] > 0:\n",
    "        df = df[df['done'] == True]\n",
    "        df = df[of_interest]\n",
    "    \n",
    "        # add debug flag to df\n",
    "        df['debug'] = d\n",
    "        # add model_name\n",
    "        df['model'] = os.path.basename(m)\n",
    "\n",
    "        # add dataset_name\n",
    "        df['dataset'] = os.path.basename(os.path.dirname(os.path.dirname(m)))\n",
    "\n",
    "        # combine into one dataset\n",
    "        large_results.append(df)\n",
    "\n",
    "large_result_df = pd.concat(large_results)\n",
    "print(large_result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"_separate_decomp\": \"separate_pca\", \"_pca\": \"pca\", \"_raw\": \"raw\"}\n",
    "modelnames = [\n",
    "            \"log_reg\",\n",
    "            \"svm\",\n",
    "            \"knn\",\n",
    "            \"rand_forest\",\n",
    "            \"decision_tree\",\n",
    "            \"xgb\",\n",
    "            \"rocket\",\n",
    "            \"kn_multivar\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result_df[\"model\"] = large_result_df[\"model\"].str.replace(\"_separate_pca\", \"_separate_decomp\")\n",
    "large_result_df[\"model\"] = large_result_df[\"model\"].str.replace(\"knn_multivar\", \"kn_multivar\")\n",
    "large_result_df[\"datamode\"] = large_result_df[\"model\"].apply(\n",
    "    lambda x: next(\n",
    "        (v for k, v in mapping.items() if k in x),\n",
    "        None  # default if no match\n",
    "    )\n",
    ")\n",
    "large_result_df[\"datamode\"]\n",
    "large_result_df[\"model\"] = large_result_df[\"model\"].apply(\n",
    "    lambda x: next(\n",
    "        (m for m in modelnames if m in x),\n",
    "        None  # default if no match\n",
    "    )\n",
    ")\n",
    "large_result_df[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = large_result_df.melt(\n",
    "    id_vars=[\"dataset\", \"debug\", \"model\", \"datamode\"],\n",
    "    value_vars=[\"mean_train_auc\", \"mean_val_auc\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"auc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all model performances on scatter plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for embedding in large_result_df.datamode.unique():\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    d = large_result_df[large_result_df.datamode == embedding]\n",
    "    g = sns.scatterplot(\n",
    "        data=d,\n",
    "        x = \"mean_train_auc\",\n",
    "        y = \"mean_val_auc\",\n",
    "        hue=\"model\",\n",
    "        style=\"dataset\",\n",
    "        s=150,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    # add y=x line\n",
    "    plt.plot([0, 1], [0, 1], ls='--', c='gray')\n",
    "    g.set_title(f\"{embedding.upper()} model performances\")\n",
    "    g.set_xlabel(\"Mean Train AUC\")\n",
    "    g.set_ylabel(\"Mean Val AUC\")\n",
    "    g.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
    "    g.set_ylim(0.3, 0.8)\n",
    "    g.set_xlim(0.5, 1.0)\n",
    "    \n",
    "    img_name = f\"{embedding}_all_models_performance.png\"\n",
    "    # plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa04ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "for ds in long_df.dataset.unique():\n",
    "    plot_df = long_df[(long_df.dataset == ds)].copy()\n",
    "    plot_df[\"model\"] = np.where(plot_df[\"debug\"], plot_df[\"model\"] + \"*\",plot_df[\"model\"])\n",
    "    # plot grouped barchart of train_auc and val_auc with model on x axis\n",
    "    if plot_df.empty:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    g = sns.catplot(\n",
    "        data=plot_df,\n",
    "        x=\"datamode\",\n",
    "        y=\"auc\",\n",
    "        hue=\"metric\",\n",
    "        col=\"model\",\n",
    "        kind=\"bar\",\n",
    "        dodge=True,\n",
    "        height=4,\n",
    "        aspect=1.2,\n",
    "        col_wrap = 3,\n",
    "        sharex=False,\n",
    "    )\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    # g.set_xticklabels(rotation=30)\n",
    "    g.set_axis_labels(\"\", \"AUC\")\n",
    "    g.set(ylim=(0, 1))\n",
    "    # g.legend.set_loc(\"upper right\")\n",
    "\n",
    "    \n",
    "    plt.suptitle(f\"{ds}\", y=1.04)\n",
    "    # plt.tight_layout()\n",
    "    # plt.legend(loc=(1,1))\n",
    "    img_name = f\"{ds}.png\"\n",
    "    # plt.savefig(os.path.join(img_dir, img_name), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top 5 per group\n",
    "for s in long_df.metric.unique():\n",
    "    print(f\"Top 5 for {s}:\")\n",
    "    print(long_df[long_df.metric == s].sort_values(by=\"auc\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5efc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which models and datasets have highest performance\n",
    "groups = large_result_df.groupby(['dataset'])\n",
    "print(groups['mean_val_auc'].max().sort_values(ascending=False)[:50])\n",
    "print(groups['mean_val_auc'].mean().sort_values(ascending=False)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215fd95",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model in model class per dataset\n",
    "groups = large_result_df.groupby(['dataset', 'model', 'datamode'])\n",
    "best_models = groups['mean_val_auc'].max().sort_values(ascending=False)\n",
    "print(best_models.reset_index()[best_models.reset_index().model == 'rocket'].mean_val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through training dirs, pick out best model\n",
    "def model_pkl_iter(dataset_names, run_name):\n",
    "    for ds in dataset_names:\n",
    "        ds_path = os.path.join(train_dir, ds)\n",
    "        for model_dir in os.listdir(ds_path):\n",
    "            if run_name in model_dir:\n",
    "                # grab model paths\n",
    "                md_path = os.path.join(ds_path, model_dir)\n",
    "                for m in os.listdir(md_path):\n",
    "                    model_path = os.path.join(md_path, m)\n",
    "                    if m.endswith(\".pkl\"):\n",
    "                        yield model_path, \"debug\" in model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ce4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# big hyper parameter plots for train and test auc\n",
    "# large_results = []\n",
    "# for m, d in model_path_iter(dataset_names, run_name):\n",
    "#     # grab results_df\n",
    "#     try:\n",
    "#         state = tune.ExperimentAnalysis(experiment_checkpoint_path=m)\n",
    "#     except ValueError:\n",
    "#         print(f\"Could not find experiment at {m}, skipping.\")\n",
    "#         continue\n",
    "#     df = state.dataframe()\n",
    "#     df = df[df[\"done\"]]\n",
    "#     hps = [c for c in df.columns if c.startswith(\"config/\") and df[c].dtype != 'object']\n",
    "#     df = pd.melt(df, id_vars=[\"mean_val_auc\", \"mean_train_auc\"], value_vars=hps, var_name=\"hp\", value_name=\"value\")\n",
    "\n",
    "#     print('\\n')\n",
    "#     print(os.path.basename(m), os.path.basename(os.path.dirname(os.path.dirname(m))))\n",
    "    \n",
    "#     plt.figure(figsize=(12,6))\n",
    "#     g = sns.FacetGrid(\n",
    "#         df,\n",
    "#         col=\"hp\",\n",
    "#         col_wrap=3,\n",
    "#         sharey=True,\n",
    "#         sharex=False\n",
    "#     )\n",
    "#     g.map_dataframe(\n",
    "#         sns.scatterplot,\n",
    "#         x=\"value\",\n",
    "#         y=\"mean_val_auc\",\n",
    "#         s=10,\n",
    "#     )\n",
    "#     g.map_dataframe(\n",
    "#         sns.scatterplot,\n",
    "#         x=\"value\",\n",
    "#         y=\"mean_train_auc\",\n",
    "#         s=10,\n",
    "#         color='red',\n",
    "#     )\n",
    "\n",
    "#     # g.set_axis_labels(x_var=\"hp\", y_var=\"mean_val_auc\")\n",
    "#     g.set_titles(col_template=\"{col_name}\", size=8)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a74ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for best models TBD\n",
    "for m, d in model_pkl_iter(dataset_names, run_name):\n",
    "    model_inst = load(open(m, \"rb\"))\n",
    "    print(type(model_inst))\n",
    "    print(model_inst.get_params())\n",
    "    # load  dataset\n",
    "\n",
    "    # split train, val\n",
    "\n",
    "\n",
    "    # fit\n",
    "\n",
    "\n",
    "    # plot confusion matrix with fitted model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9affd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
