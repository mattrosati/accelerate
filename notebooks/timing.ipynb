{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa19512",
   "metadata": {},
   "source": [
    "## Comparing Distributed ways of processing hdf5 files\n",
    "\n",
    "It would be extremely helpful if I could create a dataframe instead of looping through every file each time. The issue however is about memory, as these files would be very hard to hold all in memory. In this notebook, I will explore ways of doing this in a distributed, lazy manner. The real question is whether ``dask`` or ``polars`` would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5241d",
   "metadata": {},
   "source": [
    "#### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append(\"..\")  # add project root\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data_utils import *\n",
    "from src.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import dask\n",
    "import dask.array as da\n",
    "import polars as pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the NaturalNameWarning. Raised when attribute cannot be dot indexed because of how it is named.\n",
    "warnings.filterwarnings('ignore', category=tables.NaturalNameWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ddcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:10,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b567eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path constants\n",
    "data_dir = \"/home/mr2238/project_pi_np442/mr2238/accelerate/data\"\n",
    "img_dir = \"/home/mr2238/project_pi_np442/mr2238/accelerate/imgs/overview\"\n",
    "labels_path = os.path.join(data_dir, \"labels\")\n",
    "raw_data_path = os.path.join(data_dir, \"raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data_path = \"/home/mr2238/project_pi_np442/mr2238/accelerate/data/processed/all_data.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388828bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files\n",
    "h5py_files = [f for f in os.listdir(raw_data_path) if f.endswith(\".icmh5\")]\n",
    "print(f\"Number of h5py files: {len(h5py_files)}\")\n",
    "print(f\"Example file: {h5py_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = os.path.join(raw_data_path, h5py_files[0])\n",
    "print(example_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6053e62",
   "metadata": {},
   "source": [
    "#### Opening HDF files through ``tables``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13118d9",
   "metadata": {},
   "source": [
    "I tried this and it was super annoying to deal with .icmh5 files (would cause kernel crashes). However I realized I do not need it for ``dask``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fea5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will open with h5py instead\n",
    "global_f = h5py.File(global_data_path, mode=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a37e68",
   "metadata": {},
   "source": [
    "#### Loading Large Dataframe through ``dask``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster()          # Fully-featured local Dask cluster\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2295b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02958f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_f['1002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed092880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if I can make a dataframe of all abp values\n",
    "abp = da.concatenate([da.from_array(global_f[f\"{pt}/raw/waves/abp\"], chunks=(1e12, )) for pt in global_f.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "abp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = da.mean(abp)\n",
    "mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = ['hr', 'rso2l', 'rso2r', 'abp', 'spo2', 'icp', 'deoxhg_r',\n",
    "       'sthg_index_r', 'sthg_index_l', 'oxhg_l', 'deoxhg_l', 'oxhg_r',\n",
    "       'scthg_l', 'scthg_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed304229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "batch_n = 10\n",
    "for v in tqdm(all_vars):\n",
    "    if v in ['hr', 'rso2l', 'rso2r', 'spo2']:\n",
    "        key_string = f\"numerics/{v}\"\n",
    "    else:\n",
    "        key_string = f\"waves/{v}\"\n",
    "    arr_list = [da.from_array(global_f[f\"{pt}/raw/{key_string}\"]) for pt in global_f.keys() if key_string in global_f[f\"{pt}/raw\"] and not global_f[f\"{pt}/processed\"].attrs[\"broken_numeric\"]]\n",
    "    v_arr = da.concatenate(arr_list)\n",
    "    mini = da.concatenate(arr_list[:batch_n]).min().compute()\n",
    "    maxi = da.concatenate(arr_list)[:batch_n].max().compute()\n",
    "    hist, bins = da.histogram(v_arr, bins=50, range=[mini, maxi])\n",
    "    # print(hist)\n",
    "    hist.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267ab22",
   "metadata": {},
   "source": [
    "This takes more time than the batched approach in ``data_engineering.ipynb``. There is no need to change the approach there since this is not a speedup. This may come in handy if I ever need to do other complex things on the whole arrays, but the parallelized process appears good enough and there is no time to optimize the ``dask`` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b22cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
